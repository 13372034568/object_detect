## DataSet作为一个类，生成用于训练的groundtrue的各类label

### 训练数据的数据结构定义
```
F:\project\keras-yolo\pic_word_bg_all/1_wordandzi/1560688055.0259538782_paper.jpg 296,844,345,893,0 566,120,609,163,0 71,855,106,890,0 304,681,384,761,0 418,75,450,107,0 282,528,315,561,0 512,884,576,948,0 275,947,316,988,0 291,77,362,148,0 579,280,661,362,0 47,367,87,407,0 524,243,560,279,0 417,192,452,227,0 531,789,568,826,0 361,233,399,271,0 358,573,408,623,0
```
以上是一条真实的训练数据，由两部分构成：图片路径+标注框集合；<br>
标注框集合中，以空格区分标注框，标注框数据由四部分组成：左上角x,左上角y,右下角x,右下角y,目标类别ID；<br>
因为这里训练的数据是软笔书法，仅存在一种类型，故目标类别ID保持为0；<br>

### 预选框尺寸
涉及到两个文件 data/anchors/coco_anchors.txt 和 data/anchors/basline_anchors.txt<br>
coco_anchors.txt内容：<br>
 `10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90,  156,198,  373,326 `<br>
basline_anchors.txt内容<br>
 `1.25,1.625, 2.0,3.75, 4.125,2.875, 1.875,3.8125, 3.875,2.8125, 3.6875,7.4375, 3.625,2.8125, 4.875,6.1875, 11.65625,10.1875 `<br>
两者的联系在如下：
```
10,13, 16,30, 33,23      除以 stride=8  --> 1.25,1.625, 2.0,3.75, 4.125,2.875              （检测小目标）
30,61, 62,45, 59,119     除以 stride=16 --> 1.875,3.8125, 3.875,2.8125, 3.6875,7.4375      （检测中目标）
116,90, 156,198, 373,326 除以 stride=32 --> 3.625,2.8125, 4.875,6.1875, 11.65625,10.1875   （检测大目标）
```
coco_anchors.txt中的预选框是针对图片的原始尺寸，而basline_anchors.txt中的预选框是针对yolov3网络输出的三种特征图的尺寸，我们之所以选用basline_anchors.txt中的数据作为anchors的尺寸，后面会讲到原因

### 类别文件
文件名：data/classes/coco.names，文件内容不展示，因为需要使用预训练模型，所以这里还是使用了原始的80分类，但将软笔书法作为ID=0的分类，覆盖原来的person分类；


